# 图像模式分类

## 12.1 基础知识

图像模式分类分为 4 个主要阶段：
1. 感知：在一个二维（或更高维）空间中生成信号
2. 预处理
3. 特征提取
4. 分类

有以下 3 种基本方法：
- 原型匹配：让特征变得独特且易于检测，从而使得分类本身变简单
- 最优统计分类器：依据决策理论，通过选择参数来得到统计意义上最优的分类性能，重点在于特征选取和分类器设计
- 神经网络：使用神经网络进行分类

模式可被标记，例如字符识别中将模式标记为 0-9、a-z，也可不被标记，例如聚类后使用中心作为模式类的原型

处理被标记数据时，常将数据集分为 3 个子集：
- 训练集
- 验证集
- 测试集

使用标记数据进行训练的系统称为*有监督学习系统*，反之则是*无监督学习系统*，本章只讨论有监督学习

## 12.2 模式与模式类

图像模式主要分为 2 类：
- 定量模式：以模式向量的形式排列，模式向量由特征描述子排列而成
- 结构模式：由符号组成，以串、树或图排列，不常用，但适合用在队形状感兴趣的应用中

## 12.3 原型匹配模式分类

### 12.3.1 最小距离分类器

最小距离分类器各个模式类的原型向量通常是该类中向量的平均向量 $\boldsymbol{m}_j$，可使用欧氏距离 $D_j(\boldsymbol{x})$ 作为相似性测度
$$
\tag{12.3} D_j(\boldsymbol{x}) = \| \boldsymbol{x} - \boldsymbol{m}_j \| = \boldsymbol{x}^{\mathrm{T}} \boldsymbol{x} - (\boldsymbol{x}^{\mathrm{T}} \boldsymbol{m}_j + \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{x} - \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{m}_j) = \boldsymbol{x}^{\mathrm{T}} \boldsymbol{x} - 2(\boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{x} - \frac{1}{2} \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{m}_j)
$$
省略掉与 $j$ 无关的左侧项和以及右侧项的系数，得到以下*决策函数*（也成为*判别函数*）
$$
\tag{12.4} d_j(\boldsymbol{x}) = \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{x} - \frac{1}{2} \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{m}_j
$$
上式取得最大值时的类即为模式所属的类，当 $d_i(\boldsymbol{x}) = d_j(\boldsymbol{x})$ 时得到决策边界：
$$
\tag{12.8} d_{ij}(\boldsymbol{x}) = (\boldsymbol{m}_i - \boldsymbol{m}_j)^{\mathrm{T}} \boldsymbol{x} - \frac{1}{2} (\boldsymbol{m}_i - \boldsymbol{m}_j)^{\mathrm{T}} (\boldsymbol{m}_i + \boldsymbol{m}_j) = 0
$$

### 12.3.2 对二维原型匹配使用相关

这种方法也称为*模板匹配*

回顾 $3.43$ 中相关的定义：
$$
\tag{12.9} (w ⭐ f)(x,y) = \sum_s \sum_t w(s,t) f(x+s,y+t)
$$
> $w$ 通常称为模板，当 $w$ 与 $f$ 相同时上式也称为自相关(auto-correlation, ACORR)，不同时也称为互相关(cross-correlation, CCORR)

可使用相关系数(Correlation Coefficient, CCOEFF)并使用自相关归一化（OpenCV 中对应的方法为 `TM_CCOEFF_NORMED`）消除幅度变化的影响
$$
\tag{12.10} \gamma(x,y) = \frac{(w - \bar{w}) ⭐ (f - \bar{f})}{\sqrt{[(w - \bar{w}) ⭐ (w - \bar{w})] [(f - \bar{f}) ⭐ (f - \bar{f})]}} \in [-1,1]
$$
> 也可直接对互相关进行归一化（OpenCV 中对应的方法为 `TM_CCORR_NORMED`）

### 12.3.4 匹配结构原型

#### 1. 匹配形状数

对于两个区域边界 $a$、$b$，相似性 $k$ 定义为令它们的形状数仍保持一致的最大阶数：
$$
\tag{12.11} k = \max_j [s_j(a) = s_j(b)] \geqslant 4
$$
两个形状之间的距离定义为相似度的倒数：
$$
\tag{12.12} D(a,b) = \frac{1}{k}
$$
其满足
$$
\tag{12.13} \begin{aligned}
    D(a,b) &\geqslant 0 \\
    D(a,b) &= 0 \implies a = b \\
    D(a,c) &\leqslant \max [D(a,b), D(b,c)] \\
\end{aligned}
$$

#### 2. 串匹配

对于 2 个区域边界编码成的符号串 $\{a_1,a_2,...,a_n\}$、$\{b_1,b_2,...,b_m\}$，当 $a_k = b_k$ 时为一个匹配，匹配数量为 $\alpha$，不匹配数量为
$$
\tag{12.14} \beta = \max(m,n) - \alpha
$$
相似性为
$$
\tag{12.15} R = \frac{\alpha}{\beta} = \frac{\alpha}{\max(m,n) - \alpha}
$$

## 12.4 最优（贝叶斯，BAYES）统计分类器

是平均意义下的最优分类方法，产生错误分类的概率最低，但其依赖的概率密度函数(PDF)往往难以估计，通常假设为高斯概率密度函数

### 12.4.1 贝叶斯分类器的推导

$p(c_i / \boldsymbol{x})$ 为模式向量 $\boldsymbol{x}$ 属于类 $c_i$ 的概率，若模式分类器预测 $\boldsymbol{x}$ 属于类 $c_j$ 而它实际属于 $c_i$，则会导致一个损失 $L_{ij}$，预测产生的*条件平均风险*（简称*损失*）为
$$
\tag{12.16-17} r_j(\boldsymbol{x}) = \sum_{k=1}^{N_c} L_{kj} p(c_k / \boldsymbol{x}) = \frac{1}{p(\boldsymbol{x})} \sum_{k=1}^{N_c} L_{kj} p(\boldsymbol{x} / c_k) p(c_k)
$$
$p(\boldsymbol{x} / c_k)$ 是模式来自 $c_k$ 的概率密度函数，$p(c_k)$ 也称为*先验概率*，去掉常数项得到
$$
\tag{12.18} r_j(\boldsymbol{x}) = \sum_{k=1}^{N_c} L_{kj} p(\boldsymbol{x} / c_k) p(c_k)
$$

定义损失函数
$$
\tag{12.20} L_{ij} = \begin{cases}
    0, \quad i = j \\
    1, \quad i \ne j \\
\end{cases}
$$
带入 $12.18$ 得到
$$
\tag{12.21} r_j(\boldsymbol{x}) = p(\boldsymbol{x}) - p(\boldsymbol{x} / c_j)p(c_j)
$$
从而得到决策函数
$$
\tag{12.24} d_j(\boldsymbol{x}) = p(\boldsymbol{x} / c_j)p(c_j)
$$

### 12.4.2 高斯模式类的贝叶斯分类器

$n$ 维正态分布模式类 $c_j$ 的均值为 $\boldsymbol{m}_j$，协方差矩阵为 $\boldsymbol{C}_j$，代入 $12.24$ 可得决策函数如下
$$
\tag{12.25} d_j(\boldsymbol{x}) = \frac{1}{\sqrt{(2\pi)^n | \boldsymbol{C}_j |}} e^{-1/2 (\boldsymbol{x} - \boldsymbol{m}_j)^{\mathrm{T}} \boldsymbol{C}_j^{-1} (\boldsymbol{x} - \boldsymbol{m}_j)} p(c_j)
$$
均值和协方差矩阵定义如下
$$
\tag{11.27} \boldsymbol{m}_j = E_j\{ \boldsymbol{x} \} = \frac{1}{n_j} \sum_{\boldsymbol{x} \in c_j} \boldsymbol{x}
$$
$$
\tag{11.28} \boldsymbol{C}_j = E_j\{ (\boldsymbol{x} - \boldsymbol{m}_j)(\boldsymbol{x} - \boldsymbol{m}_j)^{\mathrm{T}} \} = \frac{1}{n_j} \sum_{\boldsymbol{x} \in c_j} \boldsymbol{x} \boldsymbol{x}^{\mathrm{T}} - \boldsymbol{m}_j \boldsymbol{m}_j^{\mathrm{T}}
$$

对 $12.25$ 右侧取对数并消除常数项可得
$$
\tag{12.33} d_j(\boldsymbol{x}) = \ln p(c_j) - \frac{1}{2} \ln | \boldsymbol{C}_j | - \frac{1}{2}[\boldsymbol{x} - \boldsymbol{m}_j)^{\mathrm{T}} \boldsymbol{C}_j^{-1} (\boldsymbol{x} - \boldsymbol{m}_j]
$$

当所有协方差矩阵相等时，消除所有不依赖 $j$ 的项，得到线性决策函数
$$
\tag{12.34} d_j(\boldsymbol{x}) = \ln p(c_j) + \boldsymbol{x}^{\mathrm{T}} \boldsymbol{C}^{-1} \boldsymbol{m}_j - \frac{1}{2} \boldsymbol{m}_j^{\mathrm{T}} \boldsymbol{C}^{-1} \boldsymbol{m}_j
$$
当 $C$ 为单位矩阵时，得到 $12.4$

## 12.5 神经网络与深度学习

### 12.5.1 背景知识

神经网络（简称网络）由大量基本非线性计算元素（称为*人工神经元*，简称神经元）以网络的形式组成，互联方式在某些方面类似于哺乳动物视觉皮层神经元

多层网络通常使用*反向传播*方法进行训练，但对于单层感知机，该方法不能收敛到一个解

神经网络不需要人工将原始数据转化为适合计算机处理的格式，而是使用反向传播从原始数据开始，自动地学习适合于识别的表示，网络中的每一层将这一表示精炼为多个更抽象的层次，这种多层学习被称为*深度学习*

图像处理中通常使用的神经网络是*卷积神经网络*，如今已成为求解复杂图像识别任务的首选方法

### 12.5.2 感知机

单个感知机单元(unit)可以学习 2 个线性可分离类的线性边界

模式列向量 $\boldsymbol{x}$ 的决策边界可用权重列向量 $\boldsymbol{w}$、偏置标量 $b$ 表示为
$$
\tag{12.38} \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x} + b = 0
$$

感知机从任意权重、偏置开始，通过迭代遍历两个类的模式，求解决策边界。如果各个类是线性可分离的，则感知机可在有限次迭代后收敛

> 实际工作中，线性可分离模式类非常罕见，它们可使用比感知机更复杂的神经网络模型进行分离

选定一个学习率 $\alpha > 0$，感知机从任意向量 $\boldsymbol{w}(1)$、任意常数 $b(1)$ 开始，对模式向量 $\boldsymbol{x}(k), k=1,2,3,...$ 执行的*感知机训练算法*如下：
1. 如果 $\boldsymbol{x}(k) \in c_1$ 且 $\boldsymbol{w}^{\mathrm{T}}(k) \boldsymbol{x}(k) + b(k) \leqslant 0$，那么令
    $$
    \tag{12.40} \begin{aligned}
        \boldsymbol{w}(k+1) &= \boldsymbol{w}(k) + \alpha \boldsymbol{x}(k) \\
        b(k+1) &= b(k) + \alpha \\
    \end{aligned}
    $$
2. 如果 $\boldsymbol{x}(k) \in c_2$ 且 $\boldsymbol{w}^{\mathrm{T}}(k) \boldsymbol{x}(k) + b(k) \geqslant 0$，那么令
    $$
    \tag{12.41} \begin{aligned}
        \boldsymbol{w}(k+1) &= \boldsymbol{w}(k) - \alpha \boldsymbol{x}(k) \\
        b(k+1) &= b(k) - \alpha \\
    \end{aligned}
    $$
3. 否则令 $\boldsymbol{w}(k+1) = \boldsymbol{w}(k)$，$b(k+1) = b(k)$

之后，感知机使用训练得到的权重、偏置，计算决策函数 $d(\boldsymbol{x}) = \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x} + b$，使用阈值处理函数作为激励函数将结果分类：
$$
c = \begin{cases}
    c_1, \quad d(\boldsymbol{x}) > 0 \\
    c_2, \quad d(\boldsymbol{x}) < 0 \\
\end{cases}
$$

令 $r \in \{ -1,1 \}$ 为感知机对模式的响应，定义增广权重向量 $\boldsymbol{w} = [w_1,w_2,...,w_n,b]^{\mathrm{T}}$，增广模式 $\boldsymbol{x} = [x_1,x_2,...,x_n,1]^{\mathrm{T}}$，则可得到如下可微且具有唯一最小值的误差函数(error function)，其取得最小值时期望响应与实际响应之间的均方误差(MSE)最小：
$$
\tag{12.47} E(\boldsymbol{w}) = \frac{1}{2}(r - \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x})^2
$$
> 神经网络中通常将误差函数称为损失函数(loss function)

可使用迭代梯度下降迭代至 $\partial E(\boldsymbol{w}) / \partial \boldsymbol{w} = 0$ 求解最小化均方误差时的权重：
$$
\tag{12.48-50} \boldsymbol{w}(k+1) = \boldsymbol{w}(k) - \alpha[\frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}}]_{\boldsymbol{w} = \boldsymbol{w}(k)} = \boldsymbol{w}(k) + \alpha [r(k) - \boldsymbol{w}^{\mathrm{T}}(k) \boldsymbol{x}(k)] \boldsymbol{x}(k)
$$

> 以上算法称为*最小均方误差*(LMSE)，收敛的必要非充分条件是 $0 < \alpha < 2$，典型的取值范围是 $0.1 < \alpha < 1.0$，实际应用中常将收敛定义为误差小于规定阈值，从而牺牲可分离性来换取收敛性

> 对于一些更复杂的分类问题，可通过增加感知机数量求解，例如异或机需要 3 个感知机，第一层由 2 个感知机接受 2 个输入模式，将 4 种可能输入减少为 2 种输出，第二层由 1 个感知机将第一层的输出进行分类

### 12.5.3 多层前馈神经网络

#### 1. 人工神经元模型

神经网络(neural network, NN)由人工神经元(artificial neurons)结合而成，其执行的计算与感知机相同，但使用不同的激励函数(activation function)

> 有时也将人工神经元称为节点

令 $h$ 代表激励函数，$(f \circ g)(x) = f[g(x)]$，$j$ 为前一层神经元的序号，$z_i^{\ell}$ 为层 $\ell$ 中第 $i$ 个神经元的计算结果（即该神经元的*净输入*或*总输入*），则该神经元的激励值为
$$
\tag{12.54-55} a_i^{\ell} = h \circ z_i^{\ell} = h \circ \sum_{j=1}^{n_{\ell-1}} \boldsymbol{w}_{ij}^{\ell} a_j^{\ell - 1} + b_i^{\ell}
$$

> 上式也称为正向传播(forward)方程，其将神经网络的输入层映射到输出层，其中常用的激励函数有：
> $$
> \begin{aligned}
>     \mathrm{Sigmoid}(z) &= \frac{1}{1 + \mathrm{e}^{-z}} \\
>     \mathrm{tanh}(z) &= \frac{\mathrm{sinh}(z)}{\mathrm{cosh}(z)} = \frac{\mathrm{e}^{2z} - 1}{\mathrm{e}^{2z} + 1} \\
>     \mathrm{ReLU}(z) &= \max(0,z) \\
> \end{aligned}
> $$
> 其中 ReLU 称为整流函数，实验结果表明其在深层神经网络中性能优于其他 2 个函数

> 网络中的神经元可以使用不同的激励函数，但没有令人信服的证据表明这样做有任何好处，因此随后讨论中默认所有神经元均使用相同的激励函数

正向传播方程中的层 $\ell$ 中所有的权重、偏置可组合成如下矩阵：
$$
\tag{12.58}
\boldsymbol{W}^{\ell} = \begin{bmatrix}
    w_{11}^{\ell} & w_{12}^{\ell} & ... & w_{1 n_{\ell - 1}}^{\ell} \\
    w_{21}^{\ell} & w_{22}^{\ell} & ... & w_{2 n_{\ell - 1}}^{\ell} \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{n_{\ell} 1}^{\ell} & w_{n_{\ell} 2}^{\ell} & ... & w_{n_{\ell} n_{\ell - 1}}^{\ell} \\
\end{bmatrix}
\quad
\boldsymbol{b}^{\ell} = \begin{bmatrix}
    b_1^{\ell} \\ b_2^{\ell} \\ \vdots \\ b_{n_{\ell}}^{\ell}
\end{bmatrix}
$$
从而可得到层 $\ell$ 输出的激励值列向量，其中 $\boldsymbol{a}^1 = \boldsymbol{x}$：
$$
\tag{12.59-60} \boldsymbol{a}^{\ell} = h \circ \boldsymbol{z}^{\ell} = h(\boldsymbol{W}^{\ell} \boldsymbol{a}^{\ell - 1} + \boldsymbol{b}^{\ell})
$$

#### 2. 互连神经元形成全连接神经网络

多层全连接神经网络的第一层的输出（激活值）是模式向量 $\boldsymbol{x}$，之后每一层由不特定数量的神经元组成，其输入是前一层的所有输出

> 全连接层(Full Connected Layer)也称为稠密层(Dense Layer)

不出现环路的网络称为*前馈网络*，本节中仅讨论全连接前馈神经网络

介于第一层和最后一层之间的神经元称为*隐藏神经元*，包含隐藏神经元的层称为*隐藏层*

> 对于模式分类应用，通常为每个输出神经元赋予一个类标记，因此有 $n_L$ 个输出神经元的网络可将模式分为 $n_L$ 个类，并将未知模式归类为拥有最大激活值的类

### 12.5.5 使用反向传播训练深层神经网络

一个神经网络由它的权重、偏置、激活函数定义，训练是用训练模式来估计这些参数的过程

训练过程中网络的输出神经元的响应可以直接得到，隐藏神经元的权重、偏置需要通过*反向传播*(backward/backpropagation)使用*损失*(loss)求解，反向传播训练的步骤如下
1. 将模式向量输入网络
2. 正向传播网络进行分类，同时得到分类损失
3. 反向传播，向网络反馈损失，计算更新参数所需的调整量
4. 更新权重和偏置
5. 重复前述步骤直至误差达到可接受水平

*损失函数*(loss function)是期望响应 $\boldsymbol{r}$ 与输出层 $\mathcal{L}$ 的实际响应 $\boldsymbol{a}^{\mathcal{L}}$ 的差别的测度

> 例如，对于 MSE 损失函数，*损失*(loss)为
> $$
> \tag{12.64-65} L = \frac{1}{n} \sum_{j=1}^n L_j = \frac{1}{n} \sum_{j=1}^n (r_j - a_j^{\mathcal{L}})^2 = \frac{1}{n} \| \boldsymbol{r} - \boldsymbol{a}^{\mathcal{L}} \| ^2
> $$
> 其中 $\boldsymbol{r}$ 中对应于 $\boldsymbol{x}$ 所属类别的元素值为 1，其他元素值为 0，这种编码称为独热码(one-hot code)

> MSE 适合用于度量回归问题，对于分类问题，更适合使用交叉熵(Cross Entropy)进行度量：
> $$
> L = - \sum_{j=1}^n r_j \log a_j^{\mathcal{L}} = - \log a_k^{\mathcal{L}}, \quad \boldsymbol{x} \in c_k
> $$

对于层 $\ell$ 中 $j$ 节点的净输入 $z_j^{\ell}$，定义 $\delta_j^{\ell} = \partial L / \partial z_j^{\ell}$，反向传播首先从最后一层 $\mathcal{L}$ 开始
$$
\tag{12.66-67} \delta_j^{\mathcal{L}}
= \frac{\partial L}{\partial z_j^{\mathcal{L}}}
= \frac{\partial L}{\partial a_j^\mathcal{L}} \frac{\partial a_j^\mathcal{L}}{\partial z_j^\mathcal{L}}
= \frac{\partial L}{\partial a_j^\mathcal{L}} \frac{\partial h(z_j^\mathcal{L})}{\partial z_j^\mathcal{L}}
= \frac{\partial L}{\partial a_j^\mathcal{L}} h'(z_j^\mathcal{L})
$$
> 例如使用 Sigmoid 激励函数，对于 MSE 损失函数
> $$
> \tag{12.68} \delta_j^{\mathcal{L}} = \frac{2(a_j^\mathcal{L} - r_j)}{n} h(z_j^\mathcal{L})[1 - h(z_j^\mathcal{L})]
> $$
> $h(z_j^\mathcal{L})$ 在正向传播中计算得到，$a_j^\mathcal{L}$ 通过网络输出得到，$r_j$ 是训练时的已知量，因此 $\delta_j^{\mathcal{L}}$ 是可计算的

之后可通过 $\delta_j^{\mathcal{L}}$ 计算 $\delta_j^{\mathcal{L} - 1} , \delta_j^{\mathcal{L} - 2} ,..., \delta_j^2$
$$
\tag{12.70} \delta_j^{\ell}
= \frac{\partial L}{\partial z_j^{\ell}}
= \sum_i \frac{\partial L}{\partial z_i^{\ell + 1}} \frac{\partial z_i^{\ell + 1}}{\partial a_j^{\ell}} \frac{\partial a_j^{\ell}}{\partial z_j^{\ell}}
= \sum_i \delta_i^{\ell + 1} \frac{\partial z_j^{\ell + 1}}{\partial a_j^{\ell}} h'(z_j^{\ell})
= h'(z_j^{\ell}) \sum_i w_{ij}^{\ell + 1} \delta_i^{\ell + 1}
$$
从而可以得到
$$
\tag{12.71-72} \begin{aligned}
    \frac{\partial L}{\partial w_{ij}^{\ell}} &= \frac{\partial L}{\partial z_i^{\ell}} \frac{\partial z_i^{\ell}}{\partial w_{ij}^{\ell}} = \delta_i^{\ell} \frac{\partial z_i^{\ell}}{\partial w_{ij}^{\ell}} = a_j^{\ell - 1} \delta_i^{\ell}
    \\
    \frac{\partial L}{\partial b_i^{\ell}} &= \delta_i^{\ell}
\end{aligned}
$$
进而可以使用学习率 $\alpha$ 更新权重和偏置
$$
\tag{12.73-74} \begin{aligned}
    w_{ij}^{\ell} &\larr w_{ij}^{\ell} - \alpha a_j^{\ell - 1} \delta_i^{\ell}
    \\
    b_i^{\ell} &\larr b_i^{\ell} - \alpha \delta_i^{\ell}
\end{aligned}
$$

令 $\odot$ 表示元素相乘，则 $12.67$ 的向量形式为
$$
\tag{12.76} \boldsymbol{\delta}^{\mathcal{L}} = \frac{\partial L}{\partial \boldsymbol{a}^{\mathcal{L}}} \odot h'(\boldsymbol{z}^{\mathcal{L}})
$$
通过将所有模式向量 $\boldsymbol{x}$ 横向级联成模式矩阵 $\boldsymbol{X}$，可得到激励值 $\boldsymbol{a}^{\mathcal{L}}$ 对应的矩阵 $\boldsymbol{A}^{\mathcal{L}}$，同理得到期望响应矩阵 $\boldsymbol{R}$、净输入矩阵 $\boldsymbol{Z}^{\mathcal{L}}$，则 $\boldsymbol{\delta}^{\mathcal{L}}$ 对应的矩阵为
$$
\tag{12.78} \boldsymbol{D}^{\mathcal{L}} = (\boldsymbol{A}^{\mathcal{L}} - \boldsymbol{R}) \odot h'(\boldsymbol{Z}^{\mathcal{L}})
$$
同样将权重向量横向级联成矩阵 $\boldsymbol{W}$，得到 $12.70$ 的矩阵形式：
$$
\tag{12.79} \boldsymbol{D}^{\ell} = [(\boldsymbol{W}^{\ell + 1})^{\mathrm{T}} \boldsymbol{D}^{\ell + 1}] \odot h'(\boldsymbol{Z}^{\ell})
$$
从而可使用学习率 $\alpha$ 更新权重和偏置
$$
\tag{12.80-81} \begin{aligned}
    \boldsymbol{W}^{\ell} &\larr \boldsymbol{W}^{\ell} - \alpha (\boldsymbol{A}^{\ell - 1})^{\mathrm{T}} \boldsymbol{D}^{\ell}
    \\
    \boldsymbol{b}^{\ell} &\larr \boldsymbol{b}^{\ell} - \alpha \sum_{k=1}^{n_p} \boldsymbol{\delta}_k^{\ell}
\end{aligned}
$$

## 12.6 深度卷积神经网络(Deep Convolutional Neural Network, CNN)

### 12.6.1 一种基本的 CNN 结构

CNN 的输入是高维阵列（称为*张量*，tensor），可以直接学习高维特征，其将张量卷积结果偏置后每个子区域（称为*感受野*，Receptive Field）对应的值激励后级联成张量（称为*特征映射*，feature map），直接馈送到下一层的对应神经元，因此其不是全连接的

> 感受野移动时的空间增量称为*步幅*(stride)，在 CNN 中，使用大于 1 的步幅可以减少数据量，同时降低对平移的敏感度

> 将卷积核视为权重张量，对于每一组权重、偏置，CNN 对所有感受野使用相同的权重、偏置，从而在所有点处检测相同的特征，该做法称为权重共享（又称参数共享）

> 通过使用多组不同的权重、偏置，可以得到多组检测不同特征的特征映射，它们可以组成更高维的特征映射张量作为下一层的输入

CNN 通常还会通过对上一层的输出进行子取样（称为*池化*，pooling）降低其对平移变化的敏感度

> 常用的池化方法有：
> 1. 平均(L1)池化
> 2. 最大池化
> 3. L2 池化

> 以 LeNet 为例，将图像作为为矩阵（即二维张量）输入的网络结构如下：
> $$
> 输入图像 \xrightarrow{卷积+偏置+激励} 特征映射 \xrightarrow{池化} 特征映射 \xrightarrow{卷积+偏置+激励} 特征映射 \xrightarrow{池化} 特征映射 \xrightarrow[向量化]{全连接神经网络} 输出
> $$
> 其中与特征映射进行卷积的张量的最低纬度的尺寸与特征映射的相同，因此激励值张量最低纬尺寸为 1，可直接降维成二维张量，然后与其他激励值张量级联成三维张量作为输出
>
> 该过程也可理解为，对于每组权重、偏置，将特征映射按特征拆分为多组二维张量，分别进行二维卷积、偏置生成多幅二维滤波图像，之后叠加为一幅二维滤波图像，将每一组的结果激励后级联成三维张量作为输出

令 $\star$ 表示卷积，$\flat$ 表示降维，$\mathbf{x}$ 为输入张量，对于第 $i$ 组权重 $\mathbf{w}_i$、偏置 $\mathbf{b}_i$，该组的输出为
$$
\tag{12.86-88} \mathbf{a}_i = h(\mathbf{z}_i) = h[(\mathbf{w}_i \star \mathbf{x})^{\flat} + \mathbf{b}_i]
$$
令 $\sharp$ 表示升维，将所有输出张量级联，得到卷积层的输出张量为
$$
\mathbf{a} 
= \{\mathbf{a}_1,\mathbf{a}_2,...,\mathbf{a}_n\}^{\sharp}
= \begin{bmatrix} \mathbf{a}_1 \\ \mathbf{a}_2 \\ \vdots \\ \mathbf{a}_n \end{bmatrix}
$$

> 对于 2 维图像处理，将每个像素点作为向量升维成 3 维张量，权重为 3 维张量，偏置为 2 维张量，每组得到的输出为 2 维张量，所有组级联为的结果为 3 维张量
>
> 在计算机处理中，通常将多幅图像（称为批，batch）级联成 3 维张量进行卷积，然后按图像拆分进行降维

### 12.6.2 CNN 的正向传播方程

将各组权重、偏置级联进行升维，通过重复对 $\mathbf{a}^{\ell - 1}$ 进行升维以使其与 $\mathbf{w}$ 最低纬度尺寸相同：
$$
\def\a{\mathbf{a}^{\ell - 1}} \a \larr \begin{bmatrix} \a \\ \a \\ \vdots \\ \a \end{bmatrix}
$$
则卷积层的正向传播方程为
$$
\tag{12.91-92} \mathbf{a}^{\ell} = h(\mathbf{z}^{\ell}) = h[(\mathbf{w}^{\ell} \star \mathbf{a}^{\ell - 1})^{\flat} + \mathbf{b}^{\ell}]
$$

$\mathbf{a}^0$ 为张量化的图像，$\mathrm{a}^{\mathcal{L}}$ 为 CNN 输出的池化特征映射

池化层仅用于减小特征映射的空间尺度，其公式随池化方法而变化

### 12.6.3 CNN 的反向传播方程

类比全连接神经网络的反向传播方程，CNN 的反向传播方程为
$$
\tag{12.95-103} \mathbf{\delta}_{x,y}^{\ell}
= \frac{\partial L}{\partial \mathbf{z}_{x,y}^{\ell}}
= h'(\mathbf{z}_{x,y}^{\ell}) \sum_u \sum_v \mathbf{\delta}_{u,v}^{\ell + 1} \mathbf{w}_{u-x,v-y}^{\ell + 1}
= h'(\mathbf{z}_{x,y}^{\ell})(\mathbf{\delta}_{x,y}^{\ell + 1} \star \mathbf{w}_{-x,-y}^{\ell + 1})
= h'(\mathbf{z}_{x,y}^{\ell})(\mathbf{\delta}_{x,y}^{\ell + 1} ⭐ \mathbf{w}^{\ell + 1})
$$
从而得到
$$
\tag{12.104-106} \begin{aligned}
    \frac{\partial L}{\partial \mathbf{w}_{l,k}^{\ell}} &= \sum_x \sum_y \mathbf{\delta}_{x,y}^{\ell} \mathbf{a}_{x-l,y-k}^{\ell - 1} = \mathbf{\delta}_{l,k}^{\ell} ⭐ \mathbf{a}^{\ell - 1}
    \\
    \frac{\partial L}{\partial \mathbf{b}^{\ell}} &= \sum_x \sum_y \mathbf{\delta}_{x,y}^{\ell}
    \end{aligned}
$$
从而可以根据以下方式更新权重、偏置
$$
\tag{12.107-108} \begin{aligned}
    \mathbf{w}_{l,k}^{\ell} &\larr \mathbf{w}_{l,k}^{\ell} - \alpha \frac{\partial L}{\partial \mathbf{w}_{l,k}^{\ell}}
    \\
    \mathbf{b}^{\ell} &\larr \mathbf{b}^{\ell} - \alpha \frac{\partial L}{\partial \mathbf{b}^{\ell}}
\end{aligned}
$$

反向传播时需要对池化层进行上取样（即复制像素）以使尺寸匹配

## 12.7 附加细节

神经网络的层数、神经元数量是个开放性问题，没有理论上的最好答案，需要通过实验探索

训练周期数(epoch)过大会导致*过拟合*(overfitting)，即网络无法泛化学到的内容，对未知模式识别率降低，可通过对*批*(batch)进行*重排*(shuffle)、对节点进行*漏失*(dropout)减缓过拟合

可通过*随机梯度下降*(Stochastic Gradient Descent, SGD)等优化器提升训练效率
